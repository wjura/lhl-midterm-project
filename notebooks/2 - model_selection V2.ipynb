{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "This notebook should include preliminary and baseline modeling.\n",
    "- Try as many different models as possible.\n",
    "- Don't worry about hyperparameter tuning or cross validation here.\n",
    "- Ideas include:\n",
    "    - linear regression\n",
    "    - support vector machines\n",
    "    - random forest\n",
    "    - xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint\n",
    "from functions_variables import encode_tags\n",
    "import seaborn as sns\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data from \n",
    "\n",
    "X_train = pd.read_csv('../data/preprocessed/X_train.csv')\n",
    "X_test = pd.read_csv('../data/preprocessed/X_test.csv')\n",
    "y_train = pd.read_csv('../data/preprocessed/y_train.csv')\n",
    "y_train = y_train.values.ravel() # Change pd df to 1d numpy array\n",
    "y_test = pd.read_csv('../data/preprocessed/y_test.csv')\n",
    "y_test = y_test.values.ravel() # Change pd df to 1d numpy array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training MSE: 20398053945.676823\n",
      "Test MSE: 185789898507.21268\n",
      "Training R^2: 0.6678487040597337\n",
      "Test R^2: 0.2962982947897199\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the data\n",
    "X_train = pd.read_csv('../data/preprocessed/X_train.csv')\n",
    "X_test = pd.read_csv('../data/preprocessed/X_test.csv')\n",
    "y_train = pd.read_csv('../data/preprocessed/y_train.csv').values.ravel()  # Convert to 1D array\n",
    "y_test = pd.read_csv('../data/preprocessed/y_test.csv').values.ravel()    # Convert to 1D array\n",
    "\n",
    "# Initialize the model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and testing data\n",
    "y_train_pred = model.predict(X_train)\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_mse = mean_squared_error(y_train, y_train_pred)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "# Output the results\n",
    "print(f\"Training MSE: {train_mse}\")\n",
    "print(f\"Test MSE: {test_mse}\")\n",
    "print(f\"Training R^2: {train_r2}\")\n",
    "print(f\"Test R^2: {test_r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge Training MSE: 20398059059.341606\n",
      "Ridge Test MSE: 185783076353.47266\n",
      "Ridge Training R^2: 0.6678486207914773\n",
      "Ridge Test R^2: 0.29632413452190465\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Initialize the Ridge model with alpha (regularization strength)\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "\n",
    "# Train the model\n",
    "ridge_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test data\n",
    "y_train_pred_ridge = ridge_model.predict(X_train)\n",
    "y_test_pred_ridge = ridge_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Ridge model\n",
    "train_mse_ridge = mean_squared_error(y_train, y_train_pred_ridge)\n",
    "test_mse_ridge = mean_squared_error(y_test, y_test_pred_ridge)\n",
    "train_r2_ridge = r2_score(y_train, y_train_pred_ridge)\n",
    "test_r2_ridge = r2_score(y_test, y_test_pred_ridge)\n",
    "\n",
    "print(f\"Ridge Training MSE: {train_mse_ridge}\")\n",
    "print(f\"Ridge Test MSE: {test_mse_ridge}\")\n",
    "print(f\"Ridge Training R^2: {train_r2_ridge}\")\n",
    "print(f\"Ridge Test R^2: {test_r2_ridge}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lasso Training MSE: 20398054631.266705\n",
      "Lasso Test MSE: 185790002907.43234\n",
      "Lasso Training R^2: 0.6678486928959448\n",
      "Lasso Test R^2: 0.2962978993612644\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "# Initialize the Lasso model with alpha (regularization strength)\n",
    "lasso_model = Lasso(alpha=1.0)\n",
    "\n",
    "# Train the model\n",
    "lasso_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on training and test data\n",
    "y_train_pred_lasso = lasso_model.predict(X_train)\n",
    "y_test_pred_lasso = lasso_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Lasso model\n",
    "train_mse_lasso = mean_squared_error(y_train, y_train_pred_lasso)\n",
    "test_mse_lasso = mean_squared_error(y_test, y_test_pred_lasso)\n",
    "train_r2_lasso = r2_score(y_train, y_train_pred_lasso)\n",
    "test_r2_lasso = r2_score(y_test, y_test_pred_lasso)\n",
    "\n",
    "print(f\"Lasso Training MSE: {train_mse_lasso}\")\n",
    "print(f\"Lasso Test MSE: {test_mse_lasso}\")\n",
    "print(f\"Lasso Training R^2: {train_r2_lasso}\")\n",
    "print(f\"Lasso Test R^2: {test_r2_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation MSE: 20879719637.5632\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='neg_mean_squared_error')\n",
    "cv_mse = -cv_scores.mean()\n",
    "\n",
    "print(f\"Cross-Validation MSE: {cv_mse}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge Regression - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Ridge: {'alpha': 100, 'solver': 'lsqr'}\n",
      "Best cross-validation score (negative MSE): -20868998146.158848\n",
      "Test MSE (Ridge): 185213017532.04303\n",
      "Test R^2 (Ridge): 0.2984833012362059\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'alpha': [0.1, 1.0, 10, 100],  # Regularization strength\n",
    "    'solver': ['auto', 'svd', 'cholesky', 'lsqr', 'saga']  # Different solvers\n",
    "}\n",
    "\n",
    "# Initialize the Ridge model\n",
    "ridge_model = Ridge()\n",
    "\n",
    "# Initialize GridSearchCV with cross-validation\n",
    "grid_search = GridSearchCV(estimator=ridge_model, param_grid=param_grid, \n",
    "                           cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best parameters for Ridge: {best_params}\")\n",
    "print(f\"Best cross-validation score (negative MSE): {best_score}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_ridge_model = grid_search.best_estimator_\n",
    "y_test_pred_ridge = best_ridge_model.predict(X_test)\n",
    "\n",
    "test_mse_ridge = mean_squared_error(y_test, y_test_pred_ridge)\n",
    "test_r2_ridge = r2_score(y_test, y_test_pred_ridge)\n",
    "\n",
    "print(f\"Test MSE (Ridge): {test_mse_ridge}\")\n",
    "print(f\"Test R^2 (Ridge): {test_r2_ridge}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lasso Regression - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Lasso: {'alpha': 100, 'max_iter': 1000}\n",
      "Best cross-validation score (negative MSE): -20870218646.206604\n",
      "Test MSE (Lasso): 185799142716.19284\n",
      "Test R^2 (Lasso): 0.29626328123045254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for Lasso\n",
    "param_grid_lasso = {\n",
    "    'alpha': [0.1, 0.5, 1.0, 5, 10, 100],  # Regularization strength\n",
    "    'max_iter': [1000, 5000, 10000],        # Maximum number of iterations\n",
    "}\n",
    "\n",
    "# Initialize the Lasso model\n",
    "lasso_model = Lasso()\n",
    "\n",
    "# Initialize GridSearchCV for Lasso\n",
    "grid_search_lasso = GridSearchCV(estimator=lasso_model, param_grid=param_grid_lasso,\n",
    "                                 cv=5, scoring='neg_mean_squared_error')\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params_lasso = grid_search_lasso.best_params_\n",
    "best_score_lasso = grid_search_lasso.best_score_\n",
    "\n",
    "print(f\"Best parameters for Lasso: {best_params_lasso}\")\n",
    "print(f\"Best cross-validation score (negative MSE): {best_score_lasso}\")\n",
    "\n",
    "# Evaluate the best Lasso model on the test set\n",
    "best_lasso_model = grid_search_lasso.best_estimator_\n",
    "y_test_pred_lasso = best_lasso_model.predict(X_test)\n",
    "\n",
    "test_mse_lasso = mean_squared_error(y_test, y_test_pred_lasso)\n",
    "test_r2_lasso = r2_score(y_test, y_test_pred_lasso)\n",
    "\n",
    "print(f\"Test MSE (Lasso): {test_mse_lasso}\")\n",
    "print(f\"Test R^2 (Lasso): {test_r2_lasso}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Polynomial Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Ridge Test MSE: 141118997708.28897\n",
      "Polynomial Ridge Test R^2: 0.46549473290641075\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a pipeline for polynomial features with Ridge or Lasso\n",
    "degree = 2  # You can try different degrees\n",
    "poly_ridge = make_pipeline(PolynomialFeatures(degree), Ridge(alpha=100))\n",
    "\n",
    "# Fit and evaluate the model\n",
    "poly_ridge.fit(X_train, y_train)\n",
    "y_test_pred_poly_ridge = poly_ridge.predict(X_test)\n",
    "\n",
    "test_mse_poly_ridge = mean_squared_error(y_test, y_test_pred_poly_ridge)\n",
    "test_r2_poly_ridge = r2_score(y_test, y_test_pred_poly_ridge)\n",
    "\n",
    "print(f\"Polynomial Ridge Test MSE: {test_mse_poly_ridge}\")\n",
    "print(f\"Polynomial Ridge Test R^2: {test_r2_poly_ridge}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Test MSE: 150821779317.79333\n",
      "Random Forest Test R^2: 0.42874427435752405\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Initialize the model\n",
    "rf_model = RandomForestRegressor(n_estimators=200, max_depth=20, random_state=42)\n",
    "\n",
    "# Fit the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_test_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "test_mse_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Test MSE: {test_mse_rf}\")\n",
    "print(f\"Random Forest Test R^2: {test_r2_rf}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestRegressor - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Best cross-validation score (negative MSE): -1977244096.4612327\n",
      "Test MSE (Random Forest): 150652485241.3763\n",
      "Test R^2 (Random Forest): 0.4293854961419906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the RandomForest model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_rf = GridSearchCV(estimator=rf_model, param_grid=param_grid_rf,\n",
    "                              cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and best score\n",
    "best_params_rf = grid_search_rf.best_params_\n",
    "best_score_rf = grid_search_rf.best_score_\n",
    "\n",
    "print(f\"Best parameters for Random Forest: {best_params_rf}\")\n",
    "print(f\"Best cross-validation score (negative MSE): {best_score_rf}\")\n",
    "\n",
    "# Evaluate the best model on the test set\n",
    "best_rf_model = grid_search_rf.best_estimator_\n",
    "y_test_pred_rf = best_rf_model.predict(X_test)\n",
    "\n",
    "test_mse_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(f\"Test MSE (Random Forest): {test_mse_rf}\")\n",
    "print(f\"Test R^2 (Random Forest): {test_r2_rf}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                         Feature    Importance\n",
      "33                                  city_encoded  3.487228e-01\n",
      "1                               description_sqft  3.307382e-01\n",
      "6                        description_total_baths  5.866685e-02\n",
      "5                  description_year_built_decade  4.043424e-02\n",
      "0                           description_lot_sqft  3.692373e-02\n",
      "2                             description_garage  2.170240e-02\n",
      "24                                     fireplace  1.782488e-02\n",
      "9                        description_type_condos  1.627777e-02\n",
      "4                               description_beds  1.453984e-02\n",
      "3                            description_stories  1.288026e-02\n",
      "31                                          view  1.134023e-02\n",
      "22                                    dishwasher  9.586357e-03\n",
      "20                   community_security_features  8.840347e-03\n",
      "12                       description_type_mobile  7.121953e-03\n",
      "26                               hardwood_floors  6.737602e-03\n",
      "17                                   central_air  6.238747e-03\n",
      "28                         recreation_facilities  6.170286e-03\n",
      "25                                    forced_air  5.207043e-03\n",
      "27                                  laundry_room  5.118253e-03\n",
      "29                                      shopping  4.671097e-03\n",
      "21                                   dining_room  4.533924e-03\n",
      "32                                  washer_dryer  4.485476e-03\n",
      "30                                  single_story  3.918862e-03\n",
      "18                                  central_heat  3.898632e-03\n",
      "19                       community_outdoor_space  3.575254e-03\n",
      "15                description_type_single_family  3.402540e-03\n",
      "23                                   family_room  3.118626e-03\n",
      "11                         description_type_land  1.379469e-03\n",
      "13                 description_type_multi_family  1.280047e-03\n",
      "16                    description_type_townhomes  6.622713e-04\n",
      "10               description_type_duplex_triplex  1.769647e-06\n",
      "8   description_type_condo_townhome_rowhome_coop  2.125873e-07\n",
      "7                         description_type_condo  0.000000e+00\n",
      "14                        description_type_other  0.000000e+00\n"
     ]
    }
   ],
   "source": [
    "# Get feature importances from the RandomForest model\n",
    "importances = best_rf_model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "feature_importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(feature_importance_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Regressor with XGBoost Test MSE: 150413526569.69757\n",
      "Stacking Regressor with XGBoost Test R^2: 0.4302905810044414\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "# Initialize base models\n",
    "rf_model = RandomForestRegressor(n_estimators=300, max_depth=20, random_state=42)\n",
    "gb_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "xgb_model = XGBRegressor(n_estimators=300, learning_rate=0.1, max_depth=3, random_state=42)\n",
    "\n",
    "# Initialize the meta-model\n",
    "meta_model = LinearRegression()\n",
    "\n",
    "# Initialize the stacking regressor\n",
    "stacking_regressor = StackingRegressor(estimators=[('rf', rf_model), ('gb', gb_model), ('xgb', xgb_model)], \n",
    "                                       final_estimator=meta_model)\n",
    "\n",
    "# Fit the stacking regressor on the training data\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_pred_stacking = stacking_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the stacking model\n",
    "test_mse_stacking = mean_squared_error(y_test, y_test_pred_stacking)\n",
    "test_r2_stacking = r2_score(y_test, y_test_pred_stacking)\n",
    "\n",
    "print(f\"Stacking Regressor with XGBoost Test MSE: {test_mse_stacking}\")\n",
    "print(f\"Stacking Regressor with XGBoost Test R^2: {test_r2_stacking}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBRegressor - GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 300, 'subsample': 1.0}\n",
      "Best cross-validation score (negative MSE): -1112311077.9447157\n",
      "Test MSE (XGBoost): 156853017253.29184\n",
      "Test R^2 (XGBoost): 0.40590023141525156\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBRegressor\n",
    "xgb_model = XGBRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb_model, param_grid=param_grid_xgb,\n",
    "                               cv=5, scoring='neg_mean_squared_error', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and score\n",
    "best_params_xgb = grid_search_xgb.best_params_\n",
    "best_score_xgb = grid_search_xgb.best_score_\n",
    "\n",
    "print(f\"Best parameters for XGBoost: {best_params_xgb}\")\n",
    "print(f\"Best cross-validation score (negative MSE): {best_score_xgb}\")\n",
    "\n",
    "# Evaluate the best XGBoost model on the test set\n",
    "best_xgb_model = grid_search_xgb.best_estimator_\n",
    "y_test_pred_xgb = best_xgb_model.predict(X_test)\n",
    "\n",
    "test_mse_xgb = mean_squared_error(y_test, y_test_pred_xgb)\n",
    "test_r2_xgb = r2_score(y_test, y_test_pred_xgb)\n",
    "\n",
    "print(f\"Test MSE (XGBoost): {test_mse_xgb}\")\n",
    "print(f\"Test R^2 (XGBoost): {test_r2_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider what metrics you want to use to evaluate success.\n",
    "- If you think about mean squared error, can we actually relate to the amount of error?\n",
    "- Try root mean squared error so that error is closer to the original units (dollars)\n",
    "- What does RMSE do to outliers?\n",
    "- Is mean absolute error a good metric for this problem?\n",
    "- What about R^2? Adjusted R^2?\n",
    "- Briefly describe your reasons for picking the metrics you use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     Model  Mean Squared Error  R^2 Score Mean Absolute Error  \\\n",
      "0  Linear Regression (All)        2.039805e+10   0.667849                None   \n",
      "1      Random Forest (All)        1.506525e+11   0.429385                None   \n",
      "2                  XGBoost        1.568530e+11   0.405900                None   \n",
      "\n",
      "   Cross-Validated MSE  \n",
      "0                  NaN  \n",
      "1        -1.977244e+09  \n",
      "2        -1.112311e+09  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define metrics for each model\n",
    "metrics = {\n",
    "    'Model': ['Linear Regression (All)', 'Random Forest (All)', 'XGBoost'],\n",
    "    'Mean Squared Error': [20398053945.676823, 150652485241.3763, 156853017253.29184],\n",
    "    'R^2 Score': [0.6678487040597337, 0.4293854961419906, 0.40590023141525156],  # R^2 for all models\n",
    "    'Mean Absolute Error': [None, None, None],  # Placeholder if no MAE provided for any model\n",
    "    'Cross-Validated MSE': [None, -1977244096.4612327, -1112311077.9447157]  # Cross-Validated MSE\n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "metrics_df = pd.DataFrame(metrics)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Summary\n",
    "Linear Regression (All) has the lowest Mean Squared Error (MSE) and highest R² score, indicating better overall fit compared to the other models.\n",
    "Random Forest and XGBoost perform similarly, but Random Forest has a slightly better R² and MSE compared to XGBoost.\n",
    "Cross-Validated MSE for Random Forest and XGBoost shows that Random Forest has a more favorable cross-validation performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection - STRETCH\n",
    "\n",
    "> **This step doesn't need to be part of your Minimum Viable Product (MVP), but its recommended you complete it if you have time!**\n",
    "\n",
    "Even with all the preprocessing we did in Notebook 1, you probably still have a lot of features. Are they all important for prediction?\n",
    "\n",
    "Investigate some feature selection algorithms (Lasso, RFE, Forward/Backward Selection)\n",
    "- Perform feature selection to get a reduced subset of your original features\n",
    "- Refit your models with this reduced dimensionality - how does performance change on your chosen metrics?\n",
    "- Based on this, should you include feature selection in your final pipeline? Explain\n",
    "\n",
    "Remember, feature selection often doesn't directly improve performance, but if performance remains the same, a simpler model is often preferrable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Lasso: {'alpha': 100}\n",
      "Lasso Test MSE: 185799142716.19284\n",
      "Lasso Test R²: 0.29626328123045254\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for Lasso\n",
    "param_grid_lasso = {'alpha': [0.01, 0.1, 1.0, 10, 100]}\n",
    "\n",
    "# Initialize the Lasso model\n",
    "lasso = Lasso()\n",
    "\n",
    "# Initialize GridSearchCV for Lasso\n",
    "grid_search_lasso = GridSearchCV(estimator=lasso, param_grid=param_grid_lasso, \n",
    "                                 cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model on training data\n",
    "grid_search_lasso.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_lasso = grid_search_lasso.best_estimator_\n",
    "print(\"Best parameters for Lasso:\", grid_search_lasso.best_params_)\n",
    "\n",
    "# Evaluate the Lasso model on test data\n",
    "y_test_pred_lasso = best_lasso.predict(X_test)\n",
    "test_mse_lasso = mean_squared_error(y_test, y_test_pred_lasso)\n",
    "test_r2_lasso = r2_score(y_test, y_test_pred_lasso)\n",
    "\n",
    "print(f\"Lasso Test MSE: {test_mse_lasso}\")\n",
    "print(f\"Lasso Test R²: {test_r2_lasso}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for Random Forest: {'max_depth': 20, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 300}\n",
      "Random Forest Test MSE: 150652485241.3763\n",
      "Random Forest Test R²: 0.4293854961419906\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid_rf = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Initialize the Random Forest Regressor\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_rf = GridSearchCV(estimator=rf, param_grid=param_grid_rf, \n",
    "                              cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search_rf.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_rf = grid_search_rf.best_estimator_\n",
    "print(\"Best parameters for Random Forest:\", grid_search_rf.best_params_)\n",
    "\n",
    "# Evaluate the Random Forest model on test data\n",
    "y_test_pred_rf = best_rf.predict(X_test)\n",
    "test_mse_rf = mean_squared_error(y_test, y_test_pred_rf)\n",
    "test_r2_rf = r2_score(y_test, y_test_pred_rf)\n",
    "\n",
    "print(f\"Random Forest Test MSE: {test_mse_rf}\")\n",
    "print(f\"Random Forest Test R²: {test_r2_rf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters for XGBoost: {'colsample_bytree': 0.8, 'learning_rate': 0.3, 'max_depth': 7, 'n_estimators': 300, 'subsample': 1.0}\n",
      "XGBoost Test MSE: 156853017253.29184\n",
      "XGBoost Test R²: 0.40590023141525156\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for XGBoost\n",
    "param_grid_xgb = {\n",
    "    'n_estimators': [100, 300, 500],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Initialize the XGBRegressor\n",
    "xgb = XGBRegressor(random_state=42)\n",
    "\n",
    "# Initialize GridSearchCV\n",
    "grid_search_xgb = GridSearchCV(estimator=xgb, param_grid=param_grid_xgb, \n",
    "                               cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "\n",
    "# Fit the model\n",
    "grid_search_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model and parameters\n",
    "best_xgb = grid_search_xgb.best_estimator_\n",
    "print(\"Best parameters for XGBoost:\", grid_search_xgb.best_params_)\n",
    "\n",
    "# Evaluate the XGBoost model on test data\n",
    "y_test_pred_xgb = best_xgb.predict(X_test)\n",
    "test_mse_xgb = mean_squared_error(y_test, y_test_pred_xgb)\n",
    "test_r2_xgb = r2_score(y_test, y_test_pred_xgb)\n",
    "\n",
    "print(f\"XGBoost Test MSE: {test_mse_xgb}\")\n",
    "print(f\"XGBoost Test R²: {test_r2_xgb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We incorporate various models (Linear Regression (Lasso), Random Forest, XGBoost) with the aim of optimizing performance to achieve the best results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "THE_ONE",
   "language": "python",
   "name": "the_one"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
